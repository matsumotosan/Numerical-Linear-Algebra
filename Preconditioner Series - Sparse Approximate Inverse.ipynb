{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preconditioner Series: Sparse Approximate Inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The premise of sparse approximate inverse (SAI) preconditioners is to explicitly calculate a sparse approximation of $A^{-1}$ such that $M\\approx A^{-1}$.\n",
    "\n",
    "Below, I implement the following approximate inverse methods:\n",
    "1. Frobenius norm minimization\n",
    "    1. SPAI algorithm\n",
    "    2. MR algorithm\n",
    "    3. Self-preconditioned MR algorithm\n",
    "2. Factorized sparse approximate inverses\n",
    "    1. Biconjugation algorithm\n",
    "3. Inverse ILU techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.cuda as tc\n",
    "from torch.autograd import Variable\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsity(M):\n",
    "    return 1 - np.count_nonzero(M.numpy()) / torch.numel(M)\n",
    "\n",
    "def report(A, M):\n",
    "    print((torch.eye(n) - torch.mm(A, M)).norm(2).item(), sparsity(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frobenius norm minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class of methods is based on the computation of a sparse matrix $M=A^{-1}$ from the following constrained minimization problem:\n",
    "\n",
    "$$ \\min_{M\\in S} \\lVert I-AM\\rVert_F$$\n",
    "\n",
    "where $S$ is a set of sparse matrices. The above problem can be solved for a right approximate inverse. The left approxiate inverse can be calculated by minimizing $\\lVert I-MA\\rVert_F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPAI algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most successful algorithms proposed in this class is known as the SPAI preconditioner. The algorithm is described [here](https://arxiv.org/abs/1503.04500) and [here](http://www.mathcs.emory.edu/~benzi/Web_papers/comp.pdf) and is as follows:\n",
    "\n",
    "For every column $m_j$ of $M$:\n",
    "\n",
    "1. Choose an initial sparsity pattern $J$\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MR algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [MR algorithm](https://www.cc.gatech.edu/~echow/pubs/newapinv.pdf)\n",
    "\n",
    "For an n-by-n matrix $A$:\n",
    "\n",
    "1. Choose an initial guess $M=M_0=[m_1, m_2,\\dots,m_n]$\n",
    "2. For each column $m_j$, $j=1,2,\\dots,n$\n",
    "    1. For $i$ in $1,2,\\dots,n_i$\n",
    "        1. $r_j=e_j-Am_j$\n",
    "        2. $\\alpha_j=r_j^TAr_j/((Ar_j)^T(Ar_j))$\n",
    "        3. $m_j=m_j+\\alpha_jr_j$\n",
    "        4. Numerical dropping on $m_j$\n",
    "        \n",
    "**Numerical dropping?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-94-b9c1c93cfe2e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-94-b9c1c93cfe2e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def mr(A, M0, ni, tol):\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def drop(m):\n",
    "    \n",
    "\n",
    "def mr(A, M, ni=10, tol=1e-3):\n",
    "    n = A.shape[0]\n",
    "    e = np.eye(n)\n",
    "    \n",
    "    for j in range(n):\n",
    "        for i in range(ni):\n",
    "            r = e[j] - A @ M[:,j]\n",
    "            a = r.T @ A @ r / ((A @ r).T @ (A @ r))\n",
    "            M[:,j] += a * r\n",
    "            drop(M[:,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "A = np.random.rand(n, n)\n",
    "M0 = np.eye(n)\n",
    "\n",
    "# mr(A, M0)\n",
    "M0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-preconditioned MR algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_mr(A, M0, ni, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorized sparse approximate inverses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biconjugation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse ILU techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain old L1-regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having learned about L1-regularization in machine learning, I wondered how such a simple method would do when calculating a sparse approximate inverse.\n",
    "\n",
    "The minimization problem here is:\n",
    "\n",
    "$$ \\min_{M} \\lVert I-AM\\rVert_F + \\alpha\\lVert M\\rVert_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(A, M, n, alpha):\n",
    "    return torch.norm(torch.mm(A, M) - torch.eye(n), p='fro') + alpha * torch.norm(M ,p=1)\n",
    "\n",
    "def plain_l1(A, lr=1e-3, alpha=1, eps=1e-3, rtol=1e-5, max_iter=10000):\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    pA = Variable(torch.FloatTensor(A), requires_grad=False)\n",
    "    pM = Variable(torch.randn(n, n), requires_grad=True)\n",
    "    \n",
    "    opt = torch.optim.SGD([pM], lr)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        opt.zero_grad()\n",
    "        pA[torch.abs(pA) < eps] = 0\n",
    "        l = loss(pA, pM, n, alpha)\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if i % 1000 == 999:\n",
    "            report(pA.data, pM.data)\n",
    "    \n",
    "    return pM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.145950794219971 0.0\n",
      "3.189674139022827 0.0\n",
      "3.157994508743286 0.0\n",
      "3.158231258392334 0.0\n",
      "3.159546375274658 0.0\n",
      "3.157010793685913 0.0\n",
      "3.1600568294525146 0.0\n",
      "3.1574387550354004 0.0\n",
      "3.158766508102417 0.0\n",
      "3.1583542823791504 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00044699,  0.00048033, -0.00070255,  0.0004818 ,  0.00092256,\n",
       "         0.00029915,  0.00062901,  0.00100516,  0.00095123,  0.00067867],\n",
       "       [ 0.00027501,  0.00001562, -0.00013192,  0.00004996,  0.00107651,\n",
       "        -0.00058413,  0.00033288,  0.00116887, -0.0001119 , -0.00003155],\n",
       "       [ 0.00082322,  0.0000967 ,  0.00074049, -0.0002428 , -0.00017854,\n",
       "        -0.00038929,  0.00034475, -0.00016088,  0.00019941,  0.00083907],\n",
       "       [-0.00005981, -0.0007041 ,  0.00091283,  0.00013724,  0.00055957,\n",
       "        -0.00050004, -0.00022909,  0.00089508,  0.00026577,  0.00085568],\n",
       "       [-0.0003328 , -0.00034273,  0.00115568,  0.00106347, -0.0007028 ,\n",
       "         0.0001009 , -0.00011733,  0.00043115, -0.00061869,  0.00048567],\n",
       "       [ 0.00084954, -0.00000967, -0.00008444, -0.00017176, -0.0001022 ,\n",
       "         0.00047752, -0.0000748 , -0.00052132,  0.00052364, -0.00071944],\n",
       "       [ 0.00035677,  0.00062591, -0.0002821 ,  0.00017212, -0.00067891,\n",
       "         0.00001098, -0.00044304,  0.00078664,  0.00030057, -0.00014486],\n",
       "       [ 0.00089441,  0.00103713, -0.00065322,  0.00000606,  0.00065853,\n",
       "        -0.00004775,  0.00093473,  0.00054724, -0.00085072,  0.00064707],\n",
       "       [ 0.00098796,  0.00113003, -0.00057535,  0.00071874, -0.00051868,\n",
       "        -0.00096723,  0.000828  , -0.00002908,  0.00116296,  0.00046364],\n",
       "       [ 0.00094571, -0.000127  , -0.00025632, -0.00083315, -0.00020889,\n",
       "        -0.00070977,  0.00051671, -0.00000468,  0.00083691, -0.00047784]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "A = np.random.rand(n, n)\n",
    "M = plain_l1(A, lr=1e-3, alpha=1, eps=1e-3)\n",
    "\n",
    "M.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A comparative study of sparse approximate inverse preconditioners](http://www.mathcs.emory.edu/~benzi/Web_papers/comp.pdf)\n",
    "\n",
    "[A Residual Based Sparse Approximate Inverse Preconditioning Procedure for Large Sparse Linear Systems](https://arxiv.org/abs/1503.04500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
